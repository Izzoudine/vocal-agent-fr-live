# ğŸ™ï¸ vocal-agent-fr-live

**Backend self-hosted complet pour agent vocal live speech-to-speech en franÃ§ais natif.**

Un agent vocal temps rÃ©el qui Ã©coute, comprend et rÃ©pond vocalement en franÃ§ais â€” 100% local, sans API cloud.

```
ğŸ¤ Vous parlez â†’ ğŸ§  STT â†’ ğŸ’¬ LLM â†’ ğŸ”Š TTS â†’ ğŸ”ˆ Il rÃ©pond vocalement
```

---

## ğŸ“‹ Table des matiÃ¨res

- [Architecture](#architecture)
- [Stack technique](#stack-technique)
- [PrÃ©requis](#prÃ©requis)
- [Installation rapide (Docker)](#installation-rapide-docker)
- [Installation manuelle](#installation-manuelle)
- [Configuration](#configuration)
- [API Reference](#api-reference)
- [Protocole WebSocket](#protocole-websocket)
- [Exemples clients](#exemples-clients)
- [DÃ©ploiement DigitalOcean + Coolify](#dÃ©ploiement-digitalocean--coolify)
- [Troubleshooting](#troubleshooting)

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Client                        â”‚
â”‚         (React / Flutter / Swift)               â”‚
â”‚                                                 â”‚
â”‚   ğŸ¤ Micro â”€â”€â†’ WebSocket â”€â”€â†’ ğŸ”ˆ Haut-parleur   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ WebSocket (binary audio + JSON)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            vocal-agent-fr-live                  â”‚
â”‚                FastAPI Server                   â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   STT    â”‚  â”‚   LLM    â”‚  â”‚     TTS      â”‚  â”‚
â”‚  â”‚ faster-  â”‚â†’ â”‚  Ollama  â”‚â†’ â”‚  MeloTTS /   â”‚  â”‚
â”‚  â”‚ whisper  â”‚  â”‚ Mistral  â”‚  â”‚  Chatterbox  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â†•                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚   mem0    â”‚                      â”‚
â”‚              â”‚  MÃ©moire  â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Ollama Server                      â”‚
â”‚         (LLM inference locale)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Stack technique

| Composant | Technologie | RÃ´le |
|-----------|-------------|------|
| **STT** | `faster-whisper` + whisper-large-v3-french | Reconnaissance vocale franÃ§ais |
| **LLM** | Ollama + Mistral 7B (ou Lucie-7B) | Intelligence conversationnelle |
| **TTS** | MeloTTS (principal) + Chatterbox (fallback) | SynthÃ¨se vocale franÃ§aise |
| **MÃ©moire** | mem0 | MÃ©moire relationnelle persistante |
| **API** | FastAPI + WebSocket | Communication temps rÃ©el |
| **Orchestration** | Pipecat | Pipeline audio |

---

## PrÃ©requis

### Minimum (CPU)
- **OS** : Linux (Ubuntu 22.04+), macOS, ou Windows (via Docker)
- **RAM** : 8 GB (avec modÃ¨les small/medium)
- **Stockage** : 50 GB SSD
- **Python** : 3.12+
- **Docker** : 24.0+

### RecommandÃ© (GPU)
- **GPU** : NVIDIA avec 8GB+ VRAM (RTX 3060+)
- **RAM** : 16 GB
- **Latence** : <1.2s glass-to-glass avec GPU

---

## Installation rapide (Docker)

```bash
# 1. Cloner le projet
git clone https://github.com/YOUR_USER/vocal-agent-fr-live.git
cd vocal-agent-fr-live

# 2. Copier et configurer l'environnement
cp .env.example .env
# Ã‰ditez .env selon vos besoins

# 3. Lancer tout avec Docker Compose
docker-compose up -d

# 4. VÃ©rifier que tout fonctionne
curl http://localhost:8765/health
```

Le premier lancement tÃ©lÃ©chargera automatiquement :
- Le modÃ¨le Ollama (~4 GB)
- Le modÃ¨le Whisper (~500 MB pour `small`)
- Le modÃ¨le MeloTTS (~500 MB)

---

## Installation manuelle

```bash
# 1. CrÃ©er un environnement virtuel
python3.12 -m venv venv
source venv/bin/activate  # Linux/macOS
# ou: .\venv\Scripts\activate  # Windows

# 2. Installer les dÃ©pendances
pip install -r requirements.txt

# 3. Installer Ollama sÃ©parÃ©ment
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull mistral:7b-instruct-v0.3-q4_0

# 4. Configurer
cp .env.example .env
# Modifiez OLLAMA_HOST=http://localhost:11434

# 5. Lancer
python main.py
```

---

## Configuration

### Variables d'environnement (.env)

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `HOST` | `0.0.0.0` | Adresse d'Ã©coute |
| `PORT` | `8765` | Port du serveur |
| `OLLAMA_HOST` | `http://ollama:11434` | URL du serveur Ollama |
| `OLLAMA_MODEL` | `mistral:7b-instruct-v0.3-q4_0` | ModÃ¨le LLM |
| `STT_MODEL_SIZE` | `small` | Taille du modÃ¨le Whisper |
| `STT_DEVICE` | `cpu` | Device STT (`cpu` / `cuda`) |
| `TTS_ENGINE` | `melo` | Moteur TTS (`melo` / `chatterbox`) |
| `TTS_VOICE_ID` | `fr_FR-melo-voice1` | Identifiant de voix |
| `MEM0_ENABLED` | `true` | Activer la mÃ©moire |
| `API_KEY` | *(vide)* | ClÃ© API optionnelle |
| `CORS_ORIGINS` | `http://localhost:3000` | Origines CORS autorisÃ©es |

### Configuration de session dynamique

Chaque session peut Ãªtre personnalisÃ©e avec :

```json
{
  "voice_id": "fr_FR-melo-voice1",
  "personality": "Tu es Eve, une coach sportive Ã©nergÃ©tique et motivante de 28 ans...",
  "situation": "Vous Ãªtes dans une salle de sport virtuelle Ã  Cotonou Ã  7h du matin, il fait 28Â°C...",
  "language": "fr-FR",
  "tts_engine": "melo",
  "user_id": "user-123"
}
```

---

## API Reference

### `GET /health`
VÃ©rification de l'Ã©tat du serveur.

```bash
curl http://localhost:8765/health
```

```json
{
  "status": "ok",
  "version": "1.0.0",
  "active_sessions": 2,
  "memory_enabled": true
}
```

### `POST /start-session`
CrÃ©er une nouvelle session vocale.

```bash
curl -X POST http://localhost:8765/start-session \
  -H "Content-Type: application/json" \
  -d '{
    "voice_id": "fr_FR-melo-voice1",
    "personality": "Tu es Eve, une coach sportive Ã©nergÃ©tique...",
    "situation": "Salle de sport virtuelle Ã  Cotonou, 7h du matin, 28Â°C",
    "language": "fr-FR",
    "tts_engine": "melo",
    "user_id": "user-123"
  }'
```

```json
{
  "session_id": "a1b2c3d4-...",
  "websocket_url": "ws://localhost:8765/ws/a1b2c3d4-...",
  "config": { ... }
}
```

### `GET /sessions`
Lister les sessions actives.

### `DELETE /sessions/{session_id}`
Supprimer une session.

---

## Protocole WebSocket

### Connexion

```
ws://localhost:8765/ws/{session_id}
```

### Messages Client â†’ Serveur

#### Audio (binaire)
Envoyez des frames audio brutes :
- **Format** : PCM 16-bit, 16kHz, mono
- **Taille recommandÃ©e** : chunks de 32000 octets (~1 seconde)

#### `session.update` (JSON)
Mettre Ã  jour la configuration en cours de session :

```json
{
  "type": "session.update",
  "voice_id": "eve-energetique",
  "personality": "Tu es un philosophe zen...",
  "situation": "MÃ©ditation guidÃ©e au lever du soleil..."
}
```

#### `input.text` (JSON)
Envoyer du texte au lieu de l'audio (bypass STT) :

```json
{
  "type": "input.text",
  "text": "Salut, comment tu vas ?"
}
```

#### `conversation.clear` (JSON)
RÃ©initialiser l'historique de conversation :

```json
{ "type": "conversation.clear" }
```

#### `memory.clear` (JSON)
Effacer la mÃ©moire de l'utilisateur :

```json
{ "type": "memory.clear" }
```

#### `ping` (JSON)
```json
{ "type": "ping" }
```

### Messages Serveur â†’ Client

#### `session.created`
```json
{
  "type": "session.created",
  "session_id": "a1b2c3d4-...",
  "config": { "voice_id": "...", "language": "fr-FR", "tts_engine": "melo" }
}
```

#### `transcription`
```json
{
  "type": "transcription",
  "text": "Salut comment Ã§a va ?",
  "is_final": true
}
```

#### `response.start` / `response.text` / `response.end`
```json
{ "type": "response.start" }
{ "type": "response.text", "text": "Ã‡a va super bien ! Et toi ?" }
{ "type": "response.end" }
```

#### `audio.start` / Audio binaire / `audio.end`
```json
{ "type": "audio.start", "sample_rate": 24000, "channels": 1 }
// ... binary audio frames (PCM 16-bit) ...
{ "type": "audio.end" }
```

#### `error`
```json
{ "type": "error", "message": "Processing error: ..." }
```

---

## Exemples clients

### React (minimal)

Voir [`examples/react-client/`](./examples/react-client/) â€” Application React complÃ¨te avec :
- Capture audio via MediaRecorder
- WebSocket bidirectionnel
- Lecture audio en streaming
- Interface visuelle

### Flutter (minimal)

Voir [`examples/flutter-client/`](./examples/flutter-client/) â€” Application Flutter avec :
- Enregistrement audio
- WebSocket via `web_socket_channel`
- Lecture audio

### Test rapide avec websocat

```bash
# Installer websocat
# https://github.com/nickel-org/websocat

# Connexion texte simple (bypass audio)
websocat ws://localhost:8765/ws/test-session

# Envoyez:
{"type": "input.text", "text": "Salut ! Tu es qui ?"}
```

### Test avec Python

```python
import asyncio
import json
import websockets

async def test():
    uri = "ws://localhost:8765/ws/test-session"
    async with websockets.connect(uri) as ws:
        # Attendez la confirmation de session
        msg = await ws.recv()
        print("Session:", json.loads(msg))

        # Envoyez un message texte
        await ws.send(json.dumps({
            "type": "input.text",
            "text": "Salut ! Raconte-moi une blague en franÃ§ais."
        }))

        # Recevez les rÃ©ponses
        while True:
            msg = await ws.recv()
            if isinstance(msg, str):
                data = json.loads(msg)
                print(f"[{data['type']}]", data.get('text', ''))
                if data['type'] == 'response.end':
                    break
            else:
                print(f"[audio] {len(msg)} bytes")

asyncio.run(test())
```

---

## DÃ©ploiement DigitalOcean + Coolify

### Option 1 : DÃ©ploiement direct sur Droplet

```bash
# 1. CrÃ©er un Droplet DigitalOcean
#    - Image : Ubuntu 24.04
#    - Plan : CPU Optimized 8GB RAM (ou GPU Droplet pour <1.2s latence)
#    - Stockage : 50GB+ SSD
#    - RÃ©gion : proche de vos utilisateurs

# 2. Se connecter au Droplet
ssh root@YOUR_DROPLET_IP

# 3. Installer Docker
curl -fsSL https://get.docker.com | sh
apt install -y docker-compose-plugin

# 4. Cloner le projet
git clone https://github.com/YOUR_USER/vocal-agent-fr-live.git
cd vocal-agent-fr-live

# 5. Configurer
cp .env.example .env
nano .env  # Ajustez les variables

# 6. Lancer
docker compose up -d

# 7. VÃ©rifier
curl http://localhost:8765/health

# 8. (Optionnel) Configurer un reverse proxy nginx
apt install -y nginx
# Configurez nginx pour proxy_pass vers localhost:8765
# avec support WebSocket (Upgrade headers)
```

### Option 2 : Via Coolify

```bash
# 1. Installer Coolify sur votre Droplet
curl -fsSL https://cdn.coollabs.io/coolify/install.sh | bash

# 2. AccÃ©der Ã  Coolify UI : http://YOUR_DROPLET_IP:8000

# 3. Ajouter une nouvelle ressource :
#    - Type : Docker Compose
#    - Source : GitHub repository
#    - Branche : main
#    - Docker Compose file : docker-compose.yml

# 4. Configurer les variables d'environnement dans Coolify UI

# 5. DÃ©ployer !
```

### GitHub Secrets necessaires

Pour le CI/CD automatique (`.github/workflows/deploy-coolify.yml`) :

| Secret | Description |
|--------|-------------|
| `DEPLOY_HOST` | IP du Droplet |
| `DEPLOY_USER` | Utilisateur SSH (ex: `root`) |
| `DEPLOY_SSH_KEY` | ClÃ© SSH privÃ©e |
| `COOLIFY_WEBHOOK_URL` | *(optionnel)* URL webhook Coolify |

---

## Troubleshooting

### Le serveur ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker-compose logs -f vocal-agent

# VÃ©rifier que Ollama est ready
curl http://localhost:11434/api/tags

# VÃ©rifier les modÃ¨les tÃ©lÃ©chargÃ©s
docker-compose logs ollama-pull
```

### Erreur "Model not found"

```bash
# TÃ©lÃ©charger le modÃ¨le manuellement
docker-compose exec ollama ollama pull mistral:7b-instruct-v0.3-q4_0
```

### MÃ©moire insuffisante (OOM)

- RÃ©duisez `STT_MODEL_SIZE` Ã  `small` ou `tiny`
- Utilisez un modÃ¨le LLM plus petit : `mistral:7b-instruct-v0.3-q4_0`
- DÃ©sactivez la mÃ©moire : `MEM0_ENABLED=false`

### Latence Ã©levÃ©e

- **CPU** : Attendez 3-8s de latence, c'est normal
- **GPU** : Passez `STT_DEVICE=cuda`, utilisez `large-v3`
- Utilisez WebRTC au lieu de WebSocket pour le transport audio

### Support GPU NVIDIA

DÃ©commentez la section GPU dans `docker-compose.yml` et installez :

```bash
# NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
apt-get update && apt-get install -y nvidia-container-toolkit
systemctl restart docker
```

---

## Licence

MIT â€” Utilisez, modifiez, distribuez librement.

---

## Contributeurs

Contributions bienvenues ! Ouvrez une issue ou un PR.
